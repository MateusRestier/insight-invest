# ü§ñ Machine Learning - Detalhamento T√©cnico

## Vis√£o Geral

O sistema utiliza **dois modelos de Random Forest**:

1. **Classificador:** Prediz se uma a√ß√£o ter√° bom desempenho futuro (Top 25%)
2. **Regressor:** Prediz o pre√ßo futuro N dias √† frente

Ambos implementam valida√ß√£o temporal rigorosa para evitar **data leakage**.

---

## 1Ô∏è‚É£ CLASSIFICADOR DE DESEMPENHO

### Objetivo

Classificar a√ß√µes em duas categorias:
- **Classe 1 (Compra):** A√ß√µes no Top 25% de retorno futuro + qualidade fundamentalista
- **Classe 0 (N√£o Compra):** A√ß√µes no Bottom 25% ou com indicadores ruins

### Rotulagem Baseada em Desempenho Futuro

#### Processo de Rotulagem

```python
# 1. Calcular data futura alvo (N dias √† frente)
df['data_futura_alvo'] = df['data_coleta'] + pd.Timedelta(days=10)

# 2. Buscar cota√ß√£o futura real (merge_asof)
df_futuro = pd.merge_asof(
    left=df[['acao', 'data_futura_alvo']],
    right=df[['acao', 'data_coleta', 'cotacao']],
    left_on='data_futura_alvo',
    right_on='data_coleta',
    by='acao',
    direction='forward'  # pega pr√≥ximo valor dispon√≠vel
)

# 3. Calcular retorno futuro
df['retorno_futuro_10_dias'] = (
    (df['preco_futuro_10_dias'] - df['cotacao']) / df['cotacao']
)

# 4. Para cada dia, calcular quantis
for data in datas_unicas:
    dia_df = df[df['data_coleta'] == data]

    q_25 = dia_df['retorno_futuro_10_dias'].quantile(0.25)  # Bottom 25%
    q_75 = dia_df['retorno_futuro_10_dias'].quantile(0.75)  # Top 25%

    # R√≥tulo 0: Bottom 25%
    df.loc[(df['data_coleta'] == data) &
           (df['retorno_futuro_10_dias'] <= q_25), 'rotulo'] = 0

    # R√≥tulo 1: Top 25% + qualidade fundamentalista
    df.loc[(df['data_coleta'] == data) &
           (df['retorno_futuro_10_dias'] >= q_75) &
           (df['pl'] > 0) & (df['roe'] > 0), 'rotulo'] = 1

    # NaN: Meio 50% (descartado)
```

#### Por que Quantis por Data?

‚úÖ **Adapta-se √† volatilidade di√°ria** (dias de alta vs baixa geral)
‚úÖ **Compara a√ß√µes entre si** naquele dia espec√≠fico
‚úÖ **Evita enviesamento** para per√≠odos espec√≠ficos
‚úÖ **Robustez temporal** (n√£o depende de valores absolutos)

### Features Engineering

#### Features Utilizadas (23 + 1 indicador)

```python
features = [
    # M√∫ltiplos de Pre√ßo
    'pl',                    # Pre√ßo/Lucro
    'pvp',                   # Pre√ßo/Valor Patrimonial

    # Rendimento
    'dividend_yield',        # % de dividendos
    'payout',               # % de distribui√ß√£o

    # Margens de Lucro
    'margem_liquida',       # % margem l√≠quida
    'margem_bruta',         # % margem bruta
    'margem_ebit',          # % margem EBIT
    'margem_ebitda',        # % margem EBITDA

    # M√∫ltiplos de Valor
    'ev_ebit',              # Enterprise Value / EBIT
    'p_ebit',               # Pre√ßo / EBIT
    'p_ativo',              # Pre√ßo / Ativos
    'p_cap_giro',           # Pre√ßo / Capital de Giro
    'p_ativo_circ_liq',     # Pre√ßo / Ativo Circulante L√≠quido

    # M√©tricas por A√ß√£o
    'vpa',                  # Valor Patrimonial por A√ß√£o
    'lpa',                  # Lucro por A√ß√£o

    # Efici√™ncia
    'giro_ativos',          # Giro dos Ativos

    # Rentabilidade
    'roe',                  # Retorno sobre Patrim√¥nio
    'roic',                 # Retorno sobre Capital Investido
    'roa',                  # Retorno sobre Ativos

    # Estrutura Financeira
    'patrimonio_ativos',    # Patrim√¥nio / Ativos
    'passivos_ativos',      # Passivos / Ativos

    # Performance de Mercado
    'variacao_12m',         # Varia√ß√£o dos √∫ltimos 12 meses

    # Feature Calculada (Value Investing)
    'preco_sobre_graham',   # Cota√ß√£o / VI_Graham

    # Indicador de Qualidade
    'fund_bad'              # 1 se PL ‚â§ 0 ou ROE ‚â§ 0
]
```

#### Feature: Pre√ßo sobre Graham

```python
# F√≥rmula de Benjamin Graham (Value Investing)
VI_Graham = sqrt(22.5 √ó LPA √ó VPA)

# Aplicado apenas se LPA > 0 e VPA > 0
preco_sobre_graham = cotacao / VI_Graham
```

**Interpreta√ß√£o:**
- `< 0.75`: Muito subavaliado (potencial de compra)
- `0.75 - 1.2`: Razoavelmente avaliado
- `> 1.5`: Sobreavaliado

#### Feature: fund_bad (Indicador de Qualidade)

```python
# Flag para empresas com indicadores ruins
fund_bad = 1 if (pl <= 0) or (roe <= 0) else 0

# For√ßa r√≥tulo 0 para a√ß√µes de baixa qualidade
if fund_bad == 1:
    rotulo = 0  # N√£o compra
```

### Valida√ß√£o Temporal Rigorosa

#### Hold-out Temporal (80/20)

```python
# Separar por data (n√£o aleat√≥rio!)
dates = df['data_coleta']
limite = dates.quantile(0.80)

# 80% dados hist√≥ricos para treino/valida√ß√£o
X_train, y_train = X[dates <= limite], y[dates <= limite]

# 20% mais recentes para avalia√ß√£o final
X_hold, y_hold = X[dates > limite], y[dates > limite]
```

**Importante:** O conjunto de hold-out **nunca √© visto durante o tuning**.

#### Cross-Validation Temporal (TimeSeriesSplit)

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)

# Exemplo com 100 amostras:
# Fold 1: treina [0:20],   valida [20:40]
# Fold 2: treina [0:40],   valida [40:60]
# Fold 3: treina [0:60],   valida [60:80]
# Fold 4: treina [0:80],   valida [80:100]
# Fold 5: treina [0:100],  valida [100:120]

# Cada fold sempre treina com passado e valida com futuro
```

**Vantagem:** Simula produ√ß√£o real onde sempre predizemos o futuro.

### Hyperparameter Tuning

#### RandomizedSearchCV

```python
param_dist = {
    'n_estimators': [50, 100, 200, 300, 400, 500],
    'max_depth': [None, 5, 10, 20, 30],
    'min_samples_leaf': [1, 2, 5],
    'max_features': ['sqrt', 'log2', 0.3, 0.5, 0.7],
    'class_weight': ['balanced', None]
}

search = RandomizedSearchCV(
    RandomForestClassifier(random_state=42),
    param_distributions=param_dist,
    n_iter=20,              # 20 combina√ß√µes aleat√≥rias
    cv=tscv,                # valida√ß√£o cruzada temporal
    scoring='roc_auc',      # m√©trica de otimiza√ß√£o
    n_jobs=-1,              # paraleliza
    random_state=42
)

search.fit(X_train, y_train)
best_model = search.best_estimator_
```

#### Por que max_features √© importante?

- **`max_features='sqrt'`:** ‚àön features por split (menos depend√™ncia de uma feature)
- **`max_features=0.5`:** 50% das features por split (for√ßa diversifica√ß√£o)

**Solu√ß√£o para "comprar no topo":** `max_features=0.5` reduz depend√™ncia de `variacao_12m`.

### M√©tricas de Avalia√ß√£o

```python
from sklearn.metrics import (
    accuracy_score,
    confusion_matrix,
    classification_report,
    roc_auc_score
)

# Predi√ß√£o no hold-out (nunca visto antes)
y_pred = modelo.predict(X_hold)
y_proba = modelo.predict_proba(X_hold)[:, 1]

# M√©tricas
accuracy = accuracy_score(y_hold, y_pred)
conf_matrix = confusion_matrix(y_hold, y_pred)
roc_auc = roc_auc_score(y_hold, y_proba)

print(f"Acur√°cia: {accuracy:.2%}")
print(f"ROC-AUC: {roc_auc:.4f}")
print(classification_report(y_hold, y_pred))
```

**M√©tricas T√≠picas:**
- Acur√°cia: 65-75%
- ROC-AUC: 0.70-0.80
- Precision (Classe 1): 70-80%
- Recall (Classe 1): 60-70%

---

## 2Ô∏è‚É£ REGRESSOR DE PRE√áOS

### Objetivo

Prever o pre√ßo de uma a√ß√£o **N dias no futuro** usando indicadores fundamentalistas atuais.

### Pipeline de Regress√£o

#### Etapa 1: Adicionar Pre√ßo Futuro

```python
def adicionar_preco_futuro(df, n_dias):
    df['data_futura_alvo'] = df['data_coleta'] + pd.Timedelta(days=n_dias)

    # Por a√ß√£o, buscar cota√ß√£o futura
    def _por_acao(grp):
        merged = pd.merge_asof(
            left=grp[['data_futura_alvo']].sort_values('data_futura_alvo'),
            right=grp[['data_coleta', 'cotacao']],
            left_on='data_futura_alvo',
            right_on='data_coleta',
            direction='forward'
        )
        grp['preco_futuro_N_dias'] = merged['cotacao'].values
        return grp

    df = df.groupby('acao', group_keys=False).apply(_por_acao)
    return df
```

#### Etapa 2: Split Temporal

```python
# Se data_calculo for hoje (produ√ß√£o)
if data_calculo > ultima_data_disponivel:
    # Treina com todos os dados
    X_train = X
    y_train = y

# Se data_calculo for hist√≥rica (avalia√ß√£o)
else:
    cutoff = pd.to_datetime(data_calculo)
    mask_train = dates < cutoff
    mask_test = dates == cutoff

    X_train, y_train = X[mask_train], y[mask_train]
    X_test, y_test = X[mask_test], y[mask_test]
```

#### Etapa 3: Treinamento

```python
model = RandomForestRegressor(
    n_estimators=100,
    random_state=42
)

model.fit(X_train, y_train)
```

### Otimiza√ß√£o Multi-Dia

#### Problema

Executar pipeline 10 vezes para prever 1, 2, 3... 10 dias:
- ‚ùå Carrega dados do banco 10 vezes
- ‚ùå Processa features 10 vezes
- ‚ùå Muito lento (~30 minutos)

#### Solu√ß√£o

```python
def executar_pipeline_multidia(max_dias=10, data_calculo=None, ...):
    # 1. Carrega dados UMA √öNICA VEZ
    df = carregar_dados_do_banco()

    # 2. Calcula features UMA √öNICA VEZ
    df = calcular_features_graham_estrito(df)

    # 3. Loop leve por horizonte
    all_predictions = []
    for n in range(1, max_dias + 1):
        # 3.1. Adiciona pre√ßo futuro para este horizonte
        df_n = adicionar_preco_futuro(df, n)

        # 3.2. Treina modelo espec√≠fico
        X_train, y_train = preparar_dados(df_n, n)
        model = RandomForestRegressor(n_estimators=100)
        model.fit(X_train, y_train)

        # 3.3. Prediz
        ultimos = X_train.groupby(acoes).tail(1)
        preds = model.predict(ultimos)

        # 3.4. Armazena
        all_predictions.append(pd.DataFrame({
            'acao': acoes,
            'data_previsao': data_calculo + timedelta(days=n),
            'preco_previsto': preds,
            'dias_a_frente': n
        }))

    # 4. Consolida tudo
    final = pd.concat(all_predictions)
    return final
```

**Resultado:** ~3-5 minutos (10x mais r√°pido!)

### M√©tricas de Avalia√ß√£o

```python
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score
)

# Predi√ß√£o
y_pred = model.predict(X_test)

# M√©tricas
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Erro percentual m√©dio
mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100

print(f"MAE: R$ {mae:.2f}")
print(f"RMSE: R$ {rmse:.2f}")
print(f"R¬≤: {r2:.4f}")
print(f"MAPE: {mape:.2f}%")
```

**M√©tricas T√≠picas:**
- MAE: R$ 0.50 - 2.00
- RMSE: R$ 1.00 - 3.00
- R¬≤: 0.85 - 0.95
- MAPE: 5-15%

---

## 3Ô∏è‚É£ RECOMENDADOR COM JUSTIFICATIVAS HEUR√çSTICAS

### N√≠veis de Recomenda√ß√£o

```python
if prob_sim >= 0.75:
    texto = "FORTEMENTE RECOMENDADA PARA COMPRA!"
elif prob_sim >= 0.60:
    texto = "RECOMENDADA PARA COMPRA"
elif prob_sim >= 0.50:
    texto = "PARCIALMENTE RECOMENDADA (Vi√©s positivo)"
elif prob_sim >= 0.40:
    texto = "PARCIALMENTE N√ÉO RECOMENDADA (Vi√©s negativo)"
elif prob_sim >= 0.25:
    texto = "N√ÉO RECOMENDADA PARA COMPRA"
else:
    texto = "FORTEMENTE N√ÉO RECOMENDADA PARA COMPRA"
```

### Filtros de Sanidade (Value Investing)

#### P/L (Pre√ßo/Lucro)

```python
if pl <= 0:
    return "‚ùå Empresa com preju√≠zo"
elif 0 < pl < 2:
    return "‚ùå Excessivamente baixo (alto risco)"
elif 2 <= pl < 10:
    return "‚úÖ Baixo, pode indicar subavalia√ß√£o"
elif 10 <= pl < 20:
    return "‚úÖ N√≠vel razo√°vel"
else:
    return "‚ö†Ô∏è Elevado"
```

#### ROE (Retorno sobre Patrim√¥nio)

```python
if roe < 0:
    return "‚ùå Negativo (preju√≠zo)"
elif 0 <= roe < 10:
    return "‚ö†Ô∏è Pode melhorar"
elif 10 <= roe < 15:
    return "‚úÖ Razo√°vel"
elif 15 <= roe < 20:
    return "‚úÖ Bom"
elif 20 <= roe <= 50:
    return "‚úÖ Excelente"
else:
    return "‚ùå Extremamente alto (suspeito de distor√ß√£o cont√°bil)"
```

#### Dividend Yield

```python
if dy < 0:
    return "‚ùå Negativo"
elif 0 <= dy < 2:
    return "‚ö†Ô∏è Baixo"
elif 2 <= dy < 4:
    return "‚úÖ Razo√°vel"
elif 4 <= dy < 6:
    return "‚úÖ Bom"
else:
    return "‚úÖ Excelente"
```

---

## Preven√ß√£o de Data Leakage - Resumo

### ‚úÖ Valida√ß√£o Temporal
- Hold-out por data (n√£o aleat√≥rio)
- TimeSeriesSplit (sempre passado ‚Üí futuro)

### ‚úÖ merge_asof com direction='forward'
- Busca cota√ß√£o futura real
- N√£o vaza informa√ß√£o do passado

### ‚úÖ Separa√ß√£o de Tuning e Avalia√ß√£o
- Tuning: Cross-validation no treino
- Avalia√ß√£o: Hold-out nunca visto

### ‚úÖ Sem Imputa√ß√£o de Dados
- Remove features com muitos missing
- Trabalha s√≥ com dados reais

---

## Feature Importance (Interpretabilidade)

```python
# Ap√≥s treinar o modelo
importances = modelo.feature_importances_
feature_names = X.columns

# Ordenar por import√¢ncia
indices = np.argsort(importances)[::-1]

print("Ranking de Features:")
for i, idx in enumerate(indices[:10], 1):
    print(f"{i}. {feature_names[idx]}: {importances[idx]:.4f}")
```

**Top Features T√≠picas:**
1. `preco_sobre_graham` (~15%)
2. `pl` (~12%)
3. `roe` (~10%)
4. `variacao_12m` (~8%)
5. `margem_liquida` (~7%)

---

## Conclus√£o

O sistema implementa Machine Learning com **rigor cient√≠fico**:

‚úÖ **Valida√ß√£o temporal** correta
‚úÖ **Feature engineering** baseado em Value Investing
‚úÖ **Tuning automatizado** com RandomizedSearchCV
‚úÖ **Interpretabilidade** via feature importances
‚úÖ **Justificativas** baseadas em heur√≠sticas de mercado
‚úÖ **M√©tricas** adequadas para cada tipo de problema